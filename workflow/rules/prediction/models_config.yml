TrainTestSplit:
  random_state: &seed 33
  test_size: 0.2
  shuffle: True

SplitterClass:
  module: sklearn.model_selection
  splitter: StratifiedShuffleSplit
  params:
    n_splits: 5
    random_state: 8

CrossValidation:
  refit: 'mean_squared_error'
  n_jobs: 8
  return_train_score: True
  verbose: 2

Models:

###########################################
# Classification (Resistant/ Susceptible) #
###########################################

# Methods included in the Scikit-Learn Package 
# For further information, please refer to https://scikit-learn.org/stable/supervised_learning.html#supervised-learning 
  
  sklearn_LR_l1:
    module: sklearn.linear_model
    model: LogisticRegression
    params:
      n_jobs: -1
      penalty: 'l1'
      class_weight: 'balanced'
      solver: 'liblinear'
    cv:
      max_iter: np.logspace(1, 5, base=10, num=5)
      solver: ['liblinear', 'saga']
      C: np.logspace(-4, 4, base=10, num=5)

  sklearn_LR_l2:
    module: sklearn.linear_model
    model: LogisticRegression
    params:
      penalty: 'l2'
      n_jobs: -1
      class_weight: 'balanced'
    cv:
      max_iter: np.logspace(1, 5, base=10, num=5)
      solver: ['newton-cg','sag','lbfgs']
      C: np.logspace(-4, 4, base=10, num=5)

  sklearn_LR_elasticnet:
    module: sklearn.linear_model
    model: LogisticRegression
    params:
      penalty: 'elasticnet'
      solver: 'saga'
      n_jobs: -1
      class_weight: 'balanced'
    cv:
      max_iter: np.logspace(1, 5, base=10, num=5)
      l1_ratio: [0.25, 0.5, 0.75]
      C: np.logspace(-4, 4, base=10, num=5)

  sklearn_SVM_l1:
    module: sklearn.svm
    model: LinearSVC
    params:
      penalty: 'l1'
      dual: False
      class_weight: 'balanced'
    cv:
      max_iter: np.logspace(1, 5, base=10, num=5)
      loss: ['squared_hinge']
      C: np.logspace(-4, 4, base=10, num=5)

  sklearn_SVM_l2:
    module: sklearn.svm
    model: LinearSVC
    params:
      penalty: 'l2'
      class_weight: 'balanced'
    cv:
      max_iter: np.logspace(1, 5, base=10, num=5)
      loss: ['squared_hinge','hinge']
      C: np.logspace(-4, 4, base=10, num=5)

  sklearn_RFC:
    module: sklearn.ensemble
    model: RandomForestClassifier
    params:
      n_jobs: -1
      class_weight: 'balanced'
    cv:
      n_estimators: [100, 300, 500, 1000]
      max_depth: [5, 10, 15, 30]
      min_samples_split: [2, 5, 10, 15]
      min_samples_leaf: [1, 2, 5, 10]

  sklearn_DT:
    module: sklearn.tree
    model: DecisionTreeClassifier
    params:
      class_weight: 'balanced'
      max_features: 'auto'
    cv:
      max_depth: [5, 10, 15, 30]
      min_samples_split: [2, 5, 10, 15]
      min_samples_leaf: [1, 2, 5, 10]
  
  sklearn_ET:
    module: sklearn.ensemble
    model: ExtraTreesClassifier
    params:
      n_jobs: -1
      class_weight: 'balanced'
    cv:
      n_estimators: [100, 300, 500, 1000]
      max_depth: [5, 10, 15, 30]
      min_samples_split: [2, 5, 10, 15]
      min_samples_leaf: [1, 2, 5, 10]

  sklearn_ADB:
    module: sklearn.ensemble
    model: AdaBoostClassifier
    params:
      n_estimators: 50
      learning_rate: 0.0001
    cv:
      n_estimators: [50, 100, 500, 1000]
      learning_rate: [0.0001, 0.001, 0.01, 0.1, 1.0]

  sklearn_GBTC:
    module: sklearn.ensemble
    model: GradientBoostingClassifier
    params:
      max_features: 'auto'
      n_iter_no_change: 20
    cv:
      learning_rate: [0.001, 0.01, 0.1, 1]
      n_estimators: [100, 300, 500, 1000]
      min_samples_split: [2, 5, 10, 15]
      max_depth: [5, 10, 15, 30]

  sklearn_SGDC_l1:
    module: sklearn.linear_model
    model: SGDClassifier
    params:
      n_jobs: -1
      class_weight: 'balanced'
    cv:
      penalty: ['l1']
      max_iter: np.logspace(1, 5, base=10, num=5)
      loss: ['hinge', 'log']
      alpha: np.logspace(-4, 0, base=10, num=5)

  sklearn_SGDC_l2:
    module: sklearn.linear_model
    model: SGDClassifier
    params:
      n_jobs: -1
      class_weight: 'balanced'
    cv:
      penalty: ['l2']
      max_iter: np.logspace(1, 5, base=10, num=5)
      loss: ['hinge', 'log']
      alpha: np.logspace(-4, 0, base=10, num=5)
    
  sklearn_SGDC_elasticnet:
    module: sklearn.linear_model
    model: SGDClassifier
    params:
      n_jobs: -1
      class_weight: 'balanced'
    cv:
      penalty: ['elasticnet']
      max_iter: np.logspace(2, 5, base=10, num=4)
      loss: ['hinge', 'log']
      alpha: np.logspace(-4, 0, base=10, num=5)
      l1_ratio: [0.25, 0.5, 0.75]

  sklearn_KNN:
    module: sklearn.neighbors
    model: KNeighborsClassifier
    params:
      n_jobs: -1
    cv:
      n_neighbors: [3,5,10,15]
      leaf_size: [20,30,40]
      p: [1,2]
      weights: ['uniform', 'distance']

  sklearn_GNB:
    module: sklearn.naive_bayes
    model: GaussianNB
    params:
      var_smoothing: 0.000001
    cv:
      var_smoothing: np.logspace(-9, 0, base=10, num=10)
  
  sklearn_CNB:
    module: sklearn.naive_bayes
    model: ComplementNB
    params:
      alpha: 0.01
    cv:
      alpha: [0.01, 0.1, 0.5, 1.0, 10.0]
      norm: [False, True]
      
# Other Methods

  INGOT:
    module: ingot
    model: INGOTClassifier
    params:
      false_positive_rate_upper_bound: 0.1
      max_rule_size: 20
      solver_name: 'PULP_CBC_CMD'
      solver_options:
        timeLimit: 1800
    cv:
      lambda_p: [0.01, 0.1, 1, 10, 100]
      lambda_z: [0.01, 0.1, 1, 10, 100]

#################################################
# Regression (Minimum Inhibitory Concentration) #
#################################################

# Methods included in the Scikit-Learn Package 
# For further information, please refer to https://scikit-learn.org/stable/supervised_learning.html#supervised-learning 
  
  sklearn_LinR:
    module: sklearn.linear_model
    model: LinearRegression
    params:
      normalize: False

  sklearn_LinR_l1:
    module: sklearn.linear_model
    model: Lasso
    params:
      alpha: 1.0
    cv:
      alpha: [0.5, 1, 2]   

  sklearn_LinR_l2:
    module: sklearn.linear_model
    model: Ridge
    params:
      alpha: 1.0
    cv:
      alpha: [0.5, 1, 2]

  sklearn_LinR_elasticnet:
    module: sklearn.linear_model
    model: ElasticNet
    params:
      alpha: 1.0
    cv:
      alpha: [0.5, 1, 2]
      l1_ratio: [0.25, 0.5, 0.75]

  sklearn_SVMR:
    module: sklearn.svm
    model: SVR
    params:
      kernel: 'rbf'
    cv:
      max_iter: np.logspace(1, 5, base=10, num=5)
      C: np.logspace(-4, 4, base=10, num=5)

  sklearn_GBTR:
    module: sklearn.ensemble
    model: GradientBoostingRegressor
    params:
      max_features: 'auto'
      n_iter_no_change: 20
    cv:
      learning_rate: [0.001, 0.01, 0.1, 1]
      n_estimators: [100, 300, 500, 1000]
      min_samples_split: [2, 5, 10, 15]
      max_depth: [5, 10, 15, 30]

  sklearn_RFR:
    module: sklearn.ensemble
    model: RandomForestRegressor
    params:
      n_jobs: -1
    cv:
      n_estimators: [100, 300, 500, 1000]
      max_depth: [5, 10, 15, 30]
      min_samples_split: [2, 5, 10, 15]
      min_samples_leaf: [1, 2, 5, 10]

  