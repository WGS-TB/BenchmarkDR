TrainTestSplit:
  random_state: &seed 33
  test_size: 0.2
  shuffle: True

SplitterClass:
  module: sklearn.model_selection
  splitter: StratifiedShuffleSplit
  params:
    n_splits: 5
    random_state: 8

CrossValidation:
  refit: 'mean_squared_error'
  n_jobs: 8
  return_train_score: True
  verbose: 2

Models:

# Classification (Resistant/ Susceptible)

  LR_l1:
    module: sklearn.linear_model
    model: LogisticRegression
    params:
      n_jobs: -1
      penalty: 'l1'
      class_weight: 'balanced'
      solver: 'liblinear'
    cv:
      max_iter: np.logspace(1, 5, base=10, num=5)
      solver: ['liblinear', 'saga']
      C: np.logspace(-4, 4, base=10, num=5)

  LR_l2:
    module: sklearn.linear_model
    model: LogisticRegression
    params:
      penalty: 'l2'
      n_jobs: -1
      class_weight: 'balanced'
    cv:
      max_iter: np.logspace(1, 5, base=10, num=5)
      solver: ['newton-cg','sag','lbfgs']
      C: np.logspace(-4, 4, base=10, num=5)

  LR_elasticnet:
    module: sklearn.linear_model
    model: LogisticRegression
    params:
      penalty: 'elasticnet'
      solver: 'saga'
      n_jobs: -1
      class_weight: 'balanced'
    cv:
      max_iter: np.logspace(1, 5, base=10, num=5)
      l1_ratio: [0.25, 0.5, 0.75]
      C: np.logspace(-4, 4, base=10, num=5)

  SVM_l1:
    module: sklearn.svm
    model: LinearSVC
    params:
      penalty: 'l1'
      dual: False
      class_weight: 'balanced'
    cv:
      max_iter: np.logspace(1, 5, base=10, num=5)
      loss: ['squared_hinge']
      C: np.logspace(-4, 4, base=10, num=5)

  SVM_l2:
    module: sklearn.svm
    model: LinearSVC
    params:
      penalty: 'l2'
      class_weight: 'balanced'
    cv:
      max_iter: np.logspace(1, 5, base=10, num=5)
      loss: ['squared_hinge','hinge']
      C: np.logspace(-4, 4, base=10, num=5)

  RF:
    module: sklearn.ensemble
    model: RandomForestClassifier
    params:
      n_jobs: -1
      class_weight: 'balanced'
    cv:
      n_estimators: [100, 300, 500, 1000]
      max_depth: [5, 10, 15, 30]
      min_samples_split: [2, 5, 10, 15]
      min_samples_leaf: [1, 2, 5, 10]

  DT:
    module: sklearn.tree
    model: DecisionTreeClassifier
    params:
      class_weight: 'balanced'
      max_features: 'auto'
    cv:
      max_depth: [5, 10, 15, 30]
      min_samples_split: [2, 5, 10, 15]
      min_samples_leaf: [1, 2, 5, 10]
  
  ET:
    module: sklearn.ensemble
    model: ExtraTreesClassifier
    params:
      n_jobs: -1
      class_weight: 'balanced'
    cv:
      n_estimators: [100, 300, 500, 1000]
      max_depth: [5, 10, 15, 30]
      min_samples_split: [2, 5, 10, 15]
      min_samples_leaf: [1, 2, 5, 10]

  ADB:
    module: sklearn.ensemble
    model: AdaBoostClassifier
    params:
      n_estimators: 50
      learning_rate: 0.0001
    cv:
      n_estimators: [50, 100, 500, 1000]
      learning_rate: [0.0001, 0.001, 0.01, 0.1, 1.0]

  GBT:
    module: sklearn.ensemble
    model: GradientBoostingClassifier
    params:
      max_features: 'auto'
    cv:
      learning_rate: [0.001, 0.01, 0.1, 1]
      n_estimators: [100, 300, 500, 1000]
      min_samples_split: [2, 5, 10, 15]
      max_depth: [5, 10, 15, 30]

  SGDC_l1:
    module: sklearn.linear_model
    model: SGDClassifier
    params:
      n_jobs: -1
      class_weight: 'balanced'
    cv:
      penalty: ['l1']
      max_iter: np.logspace(1, 5, base=10, num=5)
      loss: ['hinge', 'log']
      alpha: np.logspace(-4, 0, base=10, num=5)

  SGDC_l2:
    module: sklearn.linear_model
    model: SGDClassifier
    params:
      n_jobs: -1
      class_weight: 'balanced'
    cv:
      penalty: ['l2']
      max_iter: np.logspace(1, 5, base=10, num=5)
      loss: ['hinge', 'log']
      alpha: np.logspace(-4, 0, base=10, num=5)
    
  SGDC_elasticnet:
    module: sklearn.linear_model
    model: SGDClassifier
    params:
      n_jobs: -1
      class_weight: 'balanced'
    cv:
      penalty: ['elasticnet']
      max_iter: np.logspace(2, 5, base=10, num=4)
      loss: ['hinge', 'log']
      alpha: np.logspace(-4, 0, base=10, num=5)
      l1_ratio: [0.25, 0.5, 0.75]

  KNN:
    module: sklearn.neighbors
    model: KNeighborsClassifier
    params:
      n_jobs: -1
    cv:
      n_neighbors: [3,5,10,15]
      leaf_size: [20,30,40]
      p: [1,2]
      weights: ['uniform', 'distance']

  GNB:
    module: sklearn.naive_bayes
    model: GaussianNB
    params:
      var_smoothing: 0.000001
    cv:
      var_smoothing: np.logspace(-9, 0, base=10, num=10)
  
  CNB:
    module: sklearn.naive_bayes
    model: ComplementNB
    params:
      alpha: 0.01
    cv:
      alpha: [0.01, 0.1, 0.5, 1.0, 10.0]
      norm: [False, True]
      
  
  INGOT:
  # need to manually install via 'pip install ingotdr'
    module: ingot
    model: INGOTClassifier
    params:
      false_positive_rate_upper_bound: 0.1
      max_rule_size: 20
      solver_name: 'PULP_CBC_CMD'
      solver_options:
        timeLimit: 1800
    cv:
      lambda_p: [0.01, 0.1, 1, 10, 100]
      lambda_z: [0.01, 0.1, 1, 10, 100]

# Regression (Minimum Inhibitory Concentration)

  LinR:
    module: sklearn.linear_model
    model: LinearRegression
    params:
      normalize: False

  LinR_l1:
    module: sklearn.linear_model
    model: Lasso
    params:
      alpha: 1.0
    cv:
      alpha: [0.5, 1, 2]   

  LinR_l2:
    module: sklearn.linear_model
    model: Ridge
    params:
      alpha: 1.0
    cv:
      alpha: [0.5, 1, 2]

  LinR_elasticnet:
    module: sklearn.linear_model
    model: ElasticNet
    params:
      alpha: 1.0
    cv:
      alpha: [0.5, 1, 2]
      l1_ratio: [0.25, 0.5, 0.75]

  SVMR:
    module: sklearn.svm
    model: SVR
    params:
      kernel: 'rbf'
    cv:
      max_iter: np.logspace(1, 5, base=10, num=5)
      C: np.logspace(-4, 4, base=10, num=5)
  
#How to structure this?
  SGDR_l1:
    module: sklearn.linear_model
    model: SGDRegressor
    params:
      penalty: ['l1']
      n_jobs: -1
      max_iter: 1000
    cv:   
      loss: ['hinge', 'log']
      alpha: np.logspace(-4, 0, base=10, num=5)