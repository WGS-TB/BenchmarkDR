configfile: "config.yml"

rule build_model:
    params:
        methods=config["METHODS"]
    output:
        expand("ml/{method}.pkl",
            method=config["METHODS"])
    conda:
        "env.yml"
    shell:
        "python scripts/build_model.py {params.methods}"

rule build_model2:
    params:
        methods=config["METHODS"]
    output:
        expand("ml/{method}.joblib",
            method=config["METHODS"])
    conda:
        "env.yml"
    shell:
        "python scripts/build_model2.py --config models_conf.yml --model-name {params.methods}"

rule evaluation:
    input:
        data="data/gene_data.csv",
        label="data/AllLabels.csv",
        model="ml/{method}.joblib"
    ## TODO params based on types of evaluation: acc, auc-roc, cross validation, etc
    output:
        "results/{method}.txt"
    conda:
        "env.yml"
    shell:
        "python scripts/evaluation.py -d {input.data} -l {input.label} -m {input.model}"
